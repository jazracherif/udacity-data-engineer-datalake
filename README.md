# Udacity Data Engineer Data Lake with Spark

[staging_events]: 
[staging_songs]: 
[songs]: 
[users]: 
[time]: 
[songplay]: 
[artists]: 


In this project, I implement a datalake in S3 supported by AWS Elastic MapReduct (EMR).

Two datasets are used:
1. A song dataset, which is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/).
2. A log dataset, which consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above.

The files are stored in S3 bucket `s3a://udacity-dend` and are ingested into fact and dimension tables that are written as parquet file at location `s3://jazra-udacity-spark-etl/`

Fact Table

    songplays - records in event data associated with song plays i.e. records with page NextSong
        songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

Dimension Tables

    users - users in the app
        user_id, first_name, last_name, gender, level
    songs - songs in music database
        song_id, title, artist_id, year, duration
    artists - artists in music database
        artist_id, name, location, lattitude, longitude
    time - timestamps of records in songplays broken down into specific units
        start_time, hour, day, week, month, year, weekday

The project consists of the following files:
- etl.py: A Spark Application that runs the extraction and transformation pipeline
- submit-job.py: A script to runs the etl.py in an AWS EMR cluster.


# Installation

This project currently presupposes a running EMR cluster with the proper SSH permissions setup to access the master node

## local testing

Create the virtual environment: 

`python3 -m venv venv`

Activate the virtual environment: 

`source venv/bin/activate`

Install python packages: 

`pip3 install -r requirements.txt`

In all sections below, it is assume the venv environment is activated.


## Running the Pipeline locally on test data

The prerequisistes is to have a spark installation
`brew install apache-spark`

run the ETL in spark standalone mode:

`spark-submit etl.py`

parquet files will be generated in a new folder called `out`

to view a sample dataset from the generated tables, run:

`python test_local.py`


##  Running The Pipeline on EMR
In the following example, the master node is located at hadoop@ec2-54-153-4-183.us-west-1.compute.amazonaws.com.

to run the pipeline, copy the etl.py to the master node:

`scp etl.py hadoop@ec2-54-153-4-183.us-west-1.compute.amazonaws.com:/tmp`

Then submit the job to EMR:
`python submit-job.py`

The job can then be monitored in Spark UI at `http://ec2-54-153-4-183.us-west-1.compute.amazonaws.com:18080/history/`


Staging Tables

![staging_songs table][staging_songs]
![staging_events table][staging_events]

Fact Table

![songs table][songplay]

Dimension Tables

![songs table][songs]
![artists table][artists]
![users table][users]
![time table][time]

# Analysis


After running the pipeline, we are able to do some analysis by ingesting the S3 data using [athena](https://aws.amazon.com/athena/)

first, setup a Glue crawler that points to the bucket folder where the generated tables files are stored. The crawler will automatically extract the tables from the parquet files.

The crawlers takes a few minutes to run before the tables show up 
 
~~~ sql
WITH top_users AS (
    SELECT user_id, COUNT(*) AS count
    FROM songplay
    GROUP BY user_id
    ORDER BY cnt DESC
    LIMIT 5
)
SELECT users.first_name, 
       users.last_name, 
       top_users.cnt
  FROM top_users
 INNER JOIN users
       ON users.user_id = top_users.user_id
 ORDER BY cnt DESC
~~~~

and we get:

| first_name | last_name | cnt |
| ------------- |:-------------:|:-------------:|
| Chloe | Cuevas | 41 |
| Tegan | Levine | 31 |
| Kate | Harrell | 28 |
| Lily | Koch | 20 | 
| Aleena | Kirby | 18 |

We can also look for the top 5 most popular locations where songs are played, using the following query:

~~~ sql
SELECT location, 
       count(*) AS cnt 
  FROM songplay
 GROUP BY location 
 ORDER BY cnt DESC 
 LIMIT 5
~~~~

| location | count |
| ------------- |:-------------:|
| San Francisco-Oakland-Hayward, CA | 41
| Portland-South Portland, ME | 31
| Lansing-East Lansing, MI | 28
| Chicago-Naperville-Elgin, IL-IN-WI | 20
| Atlanta-Sandy Springs-Roswell, GA | 18

